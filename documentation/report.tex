\documentclass[letterpaper,11pt]{article}
\usepackage[utf8]{inputenc} 
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{booktabs} 
\usepackage{hyperref}
\usepackage{float} 
\usepackage{multicol}
\usepackage{enumitem}


\usetikzlibrary{shapes.geometric, arrows, positioning}
\geometry{margin=1in}

\title{\textbf{Research Project Report}}
\author{} 
\date{\today}

\begin{document}

\maketitle
\hrulefill
%\tableofcontents
\vfill

\section{Introduction}
This document will clearly outline the advancement of the research project. Based on the Scrum and sprint methodology, I will update the document every week, including what is new and what is next.
			\begin{center}
            \includegraphics[width=0.9\textwidth]{img/structure.png}
            \end{center}

\newpage
\section*{Iteration 1}
\begin{flushright}
February 3, 2025
\end{flushright}
\hrule
\vspace{0.2in}

\subsection*{What Is New?}
\begin{itemize}
    \item Created and trained a ANN MLP model for option pricing, using Black-Scholes parameters to target option prices.
            
\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/Loss_options_model.png} 
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/OPMreg.png}
  \end{minipage}
\end{figure}

    \item Compared the model's performance against Black-Scholes models:
            \begin{center}
            \includegraphics[width=0.8\textwidth]{img/BSvANN.png}
            \end{center}
            
    \item Started to build a custom LSTM model with NumPy. For now, I think Python allows better flexibility and development time than C++, while still maintaining decent performance using only NumPy. 
    I want the model to be compatible with TensorFlow formatting for easier use.
\end{itemize}

\subsection*{What Is Next?}
\begin{itemize}
    \item Finish the custom MLP model.
    \item Outliers suppression 
\end{itemize}

\newpage
\section*{Iteration 2}
\begin{flushright}
February 10, 2025
\end{flushright}
\hrule
\vspace{0.2in}
\subsection*{What Is New?}
\begin{itemize}
  \item First principle implementation of artificail neural network \textbf{multilayer perceptron}. Can be found here : /code\_/models/annModels.py
  \item \begin{verbatim}
  mlp = am.MLP(n_input=22, n_hidden1=64, n_hidden2=32, n_output=1)
  epochs = 5000
  learning_rate = 0.001
  
  #Training
  history = mlp.train(X_train_normalized, y_train, epochs, learning_rate)
  
  # Predict
  train_preds = mlp.forward(X_train_normalized)
  y_pred = mlp.forward(X_test_normalized)

  #---
  Final Training Loss: 0.41194406219492513
  Final Test Loss: 0.41460153925356924
  \end{verbatim}
  
  \begin{center}
    \includegraphics[width=0.8\textwidth]{img/custom_model_perf.png}
    \end{center}
\end{itemize}
\subsection*{What Is Next?}
\begin{itemize}
  \item Paramater optimization for custom model implementation ?
  \item Would a Transformer work better ? - \href{https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)}{Wiki Transformer}
  \begin{itemize}
    \item Very likely, However, to get a working transformer model, the data volume is much more advanced than we currently use.
  \end{itemize}
\end{itemize}

\newpage
\section*{Iteration 3}
\begin{flushright}
February 17, 2025
\end{flushright}
\hrule
\vspace{0.2in}
\subsection*{What Is New?}
\begin{itemize}
  \item Finnish data gathering with scipt :/code\_/tools/getData.ipynb, all the assets data are gather in /data/stocks (around 200 symbols)
  \item Transformer implementation in progress
  \item Benchmark against LSTM model
  \item Paramater optimization for custom model implementation ?
\end{itemize}

\subsection*{What Is Next?}
\begin{itemize}
  \item Document on maths behind models (models.pdf)
\end{itemize}


\newpage
\section*{Iteration 4}
\begin{flushright}
February 24, 2025
\end{flushright}
\hrule
\vspace{0.2in}
\subsection*{What Is New?}
\begin{itemize}
  \item LSTM implementaiton in progress
  \item FFNN MLP and LSTM mathematics models
\end{itemize}

\subsection*{What Is Next?}
\begin{itemize} 
  \item Identifies specific aspect of volatility time series (mean reversion, volatility clustering, heavy tail)
  \item identifies drawback in LSTM architecture for specific financial time series
  \item Optimize model for financial time series 
  \item identify best loss function for volatility time series
\end{itemize}





\newpage
\section*{Iteration 5}
\begin{flushright}
March 3, 2025
\end{flushright}
\hrule
\vspace{0.2in}
\subsection*{What Is New?}
\begin{itemize}
  \item Data Work\begin{itemize}
    \item Compare to litterature
    \item Normalize data to improve models performances
    \item Select relevant feature to work with the model (clean confusion matrix)
  \end{itemize}
  \item Litterature about new/modify LSTM model for financial time series prediction
  \item Document on volatility model updated
\end{itemize}


\begin{center}
  \includegraphics[width=0.7\textwidth]{img/corr_matrixS.png}
  \end{center}

\subsection*{What Is Next?}
\begin{itemize} 
  \item Improve mathematical relationship of LSTM models with litterature and financial time series properties.
  %\item 
 
\end{itemize}







\newpage
\section*{Iteration 6}
\begin{flushright}
March 10, 2025
\end{flushright}
\hrule
\vspace{0.2in}
\subsection*{What Is New?}
\begin{itemize}
  \item The volatility time series have some properties as for exemple \textbf{volatility clustering}. It implied that huge amplitude volatility periode are follow by small volatility changes and back to huge periode. The default LSTM network isn't aware of that so we can try to implement this in the forget gate to keep this informaiton inside the network.
  For instance we can propose a solution like this
  \[
  F_t = \sigma \left( W_f \cdot [H_{t-1}, X_t] + b_f \mathbf{- k \sigma_t} \right)
  \]

Where the new term $ k \sigma_t$ represente the a contante $k$ that is a leanring parameter to scale the impact on the network
and $\sigma_t$ that is the volatility estimation value.

\end{itemize}


\subsection*{What Is Next?}
\begin{itemize} 
  \item Implementaiton
  \item Benchmark against default LSTM and Black-Scholes
  \item Litterature
 
\end{itemize}




\newpage
\section*{Iteration 7}
\begin{flushright}
March 24, 2025
\end{flushright}
\hrule
\vspace{0.2in}

\subsection*{Litterature review}
\begin{enumerate}
  \item AT-LSTM: An Attention-based LSTM Model for Financial 
  Time Series Prediction\\
  \emph{Adding and attention layer to LSTM model. Appliyng weight to input feature thanks to an attention layer. Then, in a second stage, the attention model select all relevant features for LSTM input model.
  \begin{center}
    \begin{tabular}{c | c}
    \multicolumn{2}{c}{MAPE on DJIA} \\
    \hline
    LSTM & 0.00625 \\
    \hline
    AT-LSTM & 0.00486 \\
    %\hline
    \end{tabular}
  \end{center}
  }
  \item Improved Financial Predicting Method Based on Time Series Long Short-Term Memory Algorithm\\
  \emph{Automated capital prediction strategy, first by analysing the fluctuation and tail risk. Then by use ARIMA and Prophet models. Finally time series modeleing of the wavelet LSTM for a two part analysis of the linear separated wavelet and non-linear embedded wavelet to predict volatility. 
  }
  \begin{center}
    \includegraphics[width=0.7\textwidth]{img/res_lit_paper_1.png}
  \end{center}
 
  \item Prediction of Financial Time Series Based on LSTM Using Wavelet Transform and Singular Spectrum Analysis\\
  \emph{Imporove LSTM prediction capabilities by using data denoising methods including wavelet transformation (WT) and singular spectrum analysis (SSA) on the closing DJIA, divided in short, meduim and long term time periode.
  The LSTM data denoising performe better than raw data for data prediciton on all tree time periodes.
  }
  \begin{center}
    \includegraphics[width=0.7\textwidth]{img/res_lit_paper_2.png}
  \end{center}



  \item Black-Scholes-Artificial Neural Network : A novel option princing model\\
  \emph{Comparaison of multiple option pricing model and intruduction of a new model call BSANN, a basic ANN MLP model in [11-15-1] performing better than tranditionnal methodes.}

  \item Volatility forcasting using deep neural network with time-series feature embedding\\
  \emph{Propose a hybrid deep neural network model (HDNN). Encoding one-dimensionnal time-series data into two-dimensionnal GAF images to use a CNN with 2D concolutions layers, then performe feature embeding  and dense layers regression to predict the volatility}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{img/res_lit_paper_3.png}
  \end{center}


  \item Volatility forcasting using deep recurrent neural networks as GARCH models\\
  \emph{Propose new method to predict volatility time series by using a combination of GARCH and and deep neural network. Also introduce a mehanisme to identifiy ideal sliding windows side for volatilty. With evaluation of GRU, LSTM, BiLSTM}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{img/res_lit_paper_4.png}
  \end{center}

  \item Machine Learning for Options Pricing: Predicting Volatility and Optimizing Strategies – Explore how ML models can outperform   traditional pricing models (like Black-Scholes), enhancing option traders' decision-making.\\
  \emph{}


  \item NEURAL NETWORK LEARNING OF BLACK-SCHOLES
  EQUATION FOR OPTION PRICING\\
  \emph{}

  \item Option Pricing with Deep Learning\\
  \emph{This paper propose a deep learning approach to option pricing with 3 models, 2 MLP(1\&2) and a LSTM model. MPL1 as a MLP predicting the option price, while MLP2 predicting the bid \& ask of the underlying price. Furthermore, LSTM model extimating volatility to feed its outpur to the MLP1 and then having a prediction of the option price.}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{img/res_lit_paper_5.png}
  \end{center}

  \item Volatility forecast using hybrid Neural Network models \\



\end{enumerate}




\newpage
\section*{Iteration 8}
\begin{flushright}
March 31, 2025
\end{flushright}
\hrule
\vspace{0.2in}

\subsection*{LSTM implementation}
Model architecture : [8-16-1]

\subsection*{Data work}
\begin{itemize}
  \item \textbf{Re-sizing of the data} - Data work\\
  In the file dataResize.csv the size of the data have been resize to 2 weeks per exemple.
    \begin{center}
    \includegraphics[width=0.7\textwidth]{img/dataR.png}
    \end{center}

  \item \textbf{Data seasonality} - Analysis\\
  Analysis reccurent pattern in dataResize to use the 10-days seasonnality.
  \begin{figure}[H]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\linewidth]{img/10avg_vol.png} 
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.50\textwidth}
      \centering
      \includegraphics[width=\linewidth]{img/seaso.png}
    \end{minipage}
  \end{figure}



  \item \textbf{De-noising (Wavelet)} - Data work\\
  De-noising the raw data to better capture the trend in the time serie
  \begin{center}
    \includegraphics[width=0.8\textwidth]{img/wavelet.png}
    \end{center}
  \item \textbf{Sliding windows} - Analysis\\
  Papers : \href{https://link.springer.com/content/pdf/10.1007/s00180-023-01349-1.pdf}{Volatility forcasting using deep recurrent neural networks as GARCH models}\\
  and \href{https://www.sciencedirect.com/science/article/pii/S1053811920305978}{Single-scale time-dependent window-sizes in sliding-window dynamic funcitonal connectivity analysis}
  
\end{itemize}


\subsection*{Statistical models}
\begin{itemize}
  \item ARIMA
  \item GRU
  \item BiLSTM
\end{itemize}







\newpage
\section*{Iteration 9}
\begin{flushright}
April 7, 2025
\end{flushright}
\hrule
\vspace{0.2in}

%\subsection*{LSTM model}
\subsection*{Sliding window}
\subsubsection*{Implementation}
Implementation of static sliding windows on the volatility time serie 
  \begin{center}
  \includegraphics[width=0.8\textwidth]{img/win_err.png}
  \end{center} %changer la vol en close/returns
  Best window size given by the EMD is currently 5 days (a trading week) with MSE estimator.


\subsubsection*{Improvement - Dynamic sliding window}




\newpage
\section*{Iteration 10}
\begin{flushright}
April 14, 2025
\end{flushright}
\hrule
\vspace{0.2in}

\subsection*{LSTM}
Need to improve LSTM performance, not as good as TensorFlow implementation.

\subsection*{Sliding Window}
The dynamic sliding vector may cause a problem with the LSTM vectore size.



\newpage
\section*{Iteration 11}
\begin{flushright}
April 21, 2025
\end{flushright}
\hrule
\vspace{0.2in}

From the paper : \href{https://link.springer.com/content/pdf/10.1007/s00180-023-01349-1.pdf}{Volatility forcasting using deep recurrent neural networks as GARCH models}\\


\begin{enumerate}
  \item Decompose the series (returns $R_t$ and volatilities $V_t$) into $K$ intrinsic mode functions (IMFs) via EMD:
  \[
    R_t \xrightarrow{\mathrm{EMD}} \bigl\{c_i^R(t)\bigr\}_{i=1}^K,
    \qquad
    V_t \xrightarrow{\mathrm{EMD}} \bigl\{c_i^V(t)\bigr\}_{i=1}^K
  \]

  \item Hilbert‐transform each IMF to get its instantaneous phase $\phi_i(t)$, then frequency
  \[
    f_i(t)
    = \frac{1}{2\pi}\,\frac{\mathrm{d}}{\mathrm{d}t}\,\phi_i(t),
  \]
  and thus instantaneous period
  \[
    p_i(t) = \frac{1}{f_i(t)}
  \]

  \item Energy‐weight, by computing each IMF’s average energy over the sample:
  \[
    E_i = \frac{1}{T} \sum_{t=1}^{T} \bigl[c_i(t)\bigr]^2
  \]

  \item Weighted average period at time $t$:
  \[
    p^R(t)
    = \frac{1}{\sum_{i=1}^K E_i^R \sum_{i=1}^K E_i^R\,p_i^R(t)},
    \qquad
    p^V(t)
    = \frac{1}{\sum_{i=1}^K E_i^V \sum_{i=1}^K E_i^V\,p_i^V(t)}
  \]

  \item Combine returns and vol by taking the max (as the paper does) and admit $\tau $ as your ideal window size :
  \[
    \tau(t) \;=\; \max\bigl\{\,p^R(t),\,p^V(t)\bigr\}
  \]
\end{enumerate}

Another approach could be to 
\bigskip

\begin{enumerate}
  \item Compute the energie based on instantaneous amplitudes from each IMF via the Hilbert transform
  \[
    E_i = \frac{1}{T} \sum_{t=1}^{T} \bigl[c_i(t)\bigr]^2
  \]

  \item Normalize and forme energy‐based weights :
  \[
    w_R(t) \;=\; \frac{E_R(t)}{E_R(t) + E_V(t)},
    \quad
    w_V(t) \;=\; \frac{E_V(t)}{E_R(t) + E_V(t)}% \;=\; 1 - w_R(t).
  \]

  \item Weighted average of periods :
  \[
    P(t)
      \;=\;
    w_R(t)\,P^R(t)\;+\;w_V(t)\,P^V(t),
  \]
  then round up to get an integer window length:
  \[
    \tau(t)
      \;=\;
    \bigl\lceil P(t)\bigr\rceil.
  \]
\end{enumerate}


However, I am concerned that this solution limits performance by providing a smaller average window size than the previous maximization function. As a result, it may miss periodic information that the original maximization approach captured effectively.
\bigskip

With an LSTM model, a problem arises with a changing sliding window size: the dynamic changes of the input vector. Even though we can force the vector to change its size at each time point, the long-term memory information will be retained. One solution could be to modify the forget gate to enable less information retaining when the size of the window is smaller.


\begin{center}
\includegraphics[width=0.9\textwidth]{img/LSTM_archi.jpg}
\end{center}




\vfill
Related papers :
\begin{itemize}
  \item \href{https://epjnonlinearbiomedphys.springeropen.com/articles/10.1140/epjnbp/s40366-014-0014-9}{Sliding Window Empirical Mode
  Decomposition -its performance and quality}
  \item \href{https://www.mdpi.com/1996-1073/10/8/1168}{Short-Term Load Forecasting Using EMD-LSTM Neural Networks with a Xgboost Algorithm for
  Feature Importance Evaluation}
  \item \href{https://pdf.sciencedirectassets.com/271429/1-s2.0-S0306261924X00185/1-s2.0-S0306261924014405/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGwaCXVzLWVhc3QtMSJHMEUCIFoWqAhbiinl3ix6AR1mW3B5OcqVELeCM%2BvoGz0MmZW7AiEAmtvTdh2uINhXrEN%2BqF0wBYA%2Bx9ZCdChnl57cjz%2FGGJ4qvAUI9P%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDCatv51EsGNidT4vliqQBUSLAip2qgeU1wJI2NGmfJiBQ5zsCBjRD9G01%2B68Cxo3RDBiXA1LbdqT%2FMoA5EE%2Fk7dhrHLx0X1WO%2FsP2iUC7VadvgjRxJYlSAIt4qjui4k1s2vrSsv%2BIXkfFeNWSa0DPXs3exM6dJl43885vAW7%2FLTusAthmRAfdG3YuqJh9XxKq12xbrSePgAVCDX4OLNOEamtmNGdVKljJZYsCeO%2Bc87cq16UNyQDU3LQhfrcgPbuVhScdQq12WWHhlRETApEdg%2F7Tu5%2BquikxuixIavnj%2FCINDBQcpm8qE%2FSdgtA5T4yBsRrUbqjGAUWaVHXVnYqZVErq6jI1zWLYrpRqaL6V1O7gD0IcJ1MyEEXyV9fXe0M206RvFt257nbGVsySLbdCLlBUnKIGEEegCwbveiEDxwd8SK3cU7J%2Bqo6aeKd9zzsyI29YUF4Tf2iph7X3y0D3P4Gxgr3BR7OE3Q5i9nR3PDiFVAosqhk9mYwBpIw8ISAUih%2FFOsPB2bRhkiX23Vg8KXYgtuQ3qLWqNOAyMTPTlxz8WfwDbP0Brku2A%2Bq2iDhe8z3C%2BxfCOi%2FMw9YLR%2BTnGANdW0Q%2BuPDHJw61Ps3cRDvbPYjHWJH2n0tm6uu1si8QV6Pk%2FfrKNOsLCS93y%2FVyUmDCWoBQE1nogPtflYnzTHFYutVJKwCUJcEJ7AgJLY9Dc4rRu7BAxwDtGnWPBxrD7JddmL6Mj%2B1pRJxAmwGXxruc4W6ddS8N5fK7%2FVlNjbTPD%2F1BlIbeYn6KEx%2BfcAWMQtmgZqqJ7NfrMGiH%2B0nvK1qx7imCpF58G8Le86559kLHWmMTbWT5DIGF0p0968MQkxNoy2F65in75eFeSIDVhthkJ%2B0tMVfQTHLClI4gqFFML37pMAGOrEB4sLhJQY7WAhiqluDhgDpk7Mp0Rp6pq6xhpDqYoq9kXjOjDOpAPiVyJe%2FHLIs9g0qtCSfILnPYNWUK%2FOWjSL7e%2BWIBZ83v9hW0drnFW2MNNaoINarusPXKyw1LJlujqckKlqcuTrj9X9hhoHlH%2F4vjOtpjd19wdxKMiIY08Z%2F8o1tZstuMbFKOm%2FI6jL2KOZXHSeHOKpdN0L6U3DKqDVYOUgGP94xCUmR6VtGwou3JMB3&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250423T195138Z&X-Amz-SignedHeaders=host&X-Amz-Expires=299&X-Amz-Credential=ASIAQ3PHCVTYYJXNQUA7%2F20250423%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=171ce7e9ef2a61c822ab5e836113a04ac881d24638bfcafecc422ebef0b35a65&hash=2de7ac10a381a46f6e2ea4319ed1f57aa7d11df62c61029e5a3f907fa2db6fa0&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0306261924014405&tid=spdf-51a7ca99-a11b-47ef-9468-cac492f2964d&sid=85df26183780b241fd383b000bed50d4cd65gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=11105f575606005558&rr=934fe58fb82afa06&cc=us}{An attention-based multi-input LSTM with sliding window-based two-stage
decomposition for wind speed forecasting}
  
\end{itemize}




\newpage
\section*{Iteration 12}
\begin{flushright}
April 28, 2025
\end{flushright}
\hrule
\vspace{0.2in}

Comparaison between different methods to choose ideal size of the sliding window ; default, EMD and proposed :

\bigskip
\[
\begin{array}{cccc}
Metric/Method & Default & EMD & Proposed \\
MSE & 0.002921 & 0.003081 & 0.003104 \\
MAE & 0.038735 & 0.039244 & 0.039849 \\
MAPE & 60.86 & 55.60 & 64.80 \\
R^{2} & 0.4224 & 0.4712 & 0.2427
\end{array}
\]


\bigskip
No time to look the density metrics foracting idea




\newpage
\section*{Iteration 13}
\begin{flushright}
May 12, 2025
\end{flushright}
\hrule
\vspace{0.2in}

\subsection*{Weight EMD model}
Write the model for each IMF instead of the global static one size window.
  \[
    E_i = \frac{1}{T} \bigl[c_i(t)\bigr]^2
  \]

  \bigskip

  \[
    P^R(t) = \frac{1}{E_i^R E_i^R\,P_i^R(t)},
    \qquad
    P^V(t) = \frac{1}{E_i^V E_i^V\,P_i^V(t)}
  \]

  \bigskip

  \[
    \tau(t) \;=\; \max\bigl\{\,P^R(t),\,P^V(t)\bigr\}
  \]


\subsection*{Forget gate modification to enable dynamic LSTM model}
On way to approach this probleme could be to change the long term memory proportionnaly to the size of the window by weighting it.
For exemple a deacresing window, retaining less information further back in the past would induced a long term memory value weighted less.
The weights can be calculated by the ration of the changing size.\\


The forget gate could change from :
\[
\mathbf{F}_t = \sigma(\mathbf{W}_f \cdot [\mathbf{H}_{t-1}, \mathbf{X}_t] + \mathbf{b}_f)
\]

To :

%\[
%\mathbf{F}_t = \sigma(\frac{\mathbf{S}_{t}}{\mathbf{S}_{t-1}}(\mathbf{W}_f \cdot [\mathbf{H}_{t-1}, \mathbf{X}_t] + \mathbf{b}_f))
%\]


\[
\mathbf{F}_t = \sigma(\mathbf{W}_f \cdot [\frac{\mathbf{H}_{t-1}}{\frac{\mathbf{S}_{t-1}}{\mathbf{S}_{t}}}, \mathbf{X}_t] + \mathbf{b}_f)
\]

With $\mathbf{S}$ the size of the sliding window.
\bigskip

Test result : 

\bigskip
\[
\begin{array}{ccc}
Metric/Method & Base & Proposed \\
\hline
MSE & 0.003 & 0.056\\
MAE & 0.040 & 0.038 \\
MAPE & 63.31\% & 48.34\%  \\
R^{2} & 0.457 & 0.3703 
\end{array}
\]

\bigskip

Regarding these result, it appear the proposed method can improve accuracy for most of the point, however, looking at the MSE and R² metric some few big misses are occuring. The process show improvement but need to be more stable to correctly predicte all values without big misses that dragues metrics the wrong way.




\subsection*{Probability density model}
Using density distribution of the volatility to improve forecasting. By giving a model the caracteristics of the probability density function, ($\sigma$, $\mu$, mediane, mode, etc...)




\newpage
\section*{Iteration 14}
\begin{flushright}
May 19, 2025
\end{flushright}
\hrule
\vspace{0.2in}

\subsection*{Forget gate}
Controling the information retention by replacing with zero some memory information.

Let's take the input vector :
\[
(seq\_lenght, batch\_size, input\_size)
\]

Where :
\begin{itemize}
  \item $seq\_lenght$ : the size of the time serie data point fed to the model (sliding window size)
  \item $batch\_size$ : the size of the batch for each weight and bias update
  \item $input\_size$ : number of feature fed to the model (in our case 1 as there is only volatility time serie for now)
\end{itemize}

\bigskip
In order to dynamicly adjust the $seq\_lenght$ we need to set a $max\_seq\_lenght$ as we can decrease the vectore size but not increase it.

For a given vectore :
\[
[1; 2; 3; 4; 5]
\]


Admit $max\_seq\_lenght = 3 $, let's say in a time step the ideal sliding window size is 2 we set $seq\_lenght = 2$ so the vector changes to :
\[
\begin{array}{cccc}
1 & 2 & 3 & \text{seq} = 3 \\
0 & 3 & 4 & \text{seq} = 2 \\
3 & 4 & 5 & \text{seq} = 3 \\
\end{array}
\]


\bigskip

Test result of the implementation : 

\bigskip
\[
\begin{array}{ccc}
Metric/Method & Base & New \\
\hline
MSE & 0.003 & 0.003\\
MAE & 0.040 & 0.049 \\
MAPE & 60.76\% & 97.64\%  \\
R^{2} & 0.465 & 0.320 
\end{array}
\]

\bigskip


We also have to keep in mind that the computing time of the new methode is much higher, (around 8 minutes here against 30 sec)







\newpage
\section*{Iteration 15}
\begin{flushright}
May 26, 2025
\end{flushright}
\hrule
\vspace{0.2in}


\subsection*{Index masking for LSTM model}
Here the goal is to add a discreat value, either $[0;1]$ for now to allow the total forgetting of time serie value, in our case ie, the value associate with 0.\\
For exemple : 
\[
\begin{array}{cccc}
0 & 0 & 1 & 1\\
3 & 4 & 5 & 6 \\
\end{array}
\]

In that case, 3 and 4 will be forgotten by the model, while still being in a vector of size 4.\\

\bigskip

The input tensor now is a 4d matrix in order to add this information to each data point.
\[
(seq\_lenght, batch\_size, input\_size, index\_mask)
\]

After implementation :
\bigskip
\[
\begin{array}{ccc}
Metric/Method & Base & Masking \\
\hline
MSE & 0.003 & 0.002\\
MAE & 0.040 & 0.038 \\
MAPE & 60.76\% & 55.76\%  \\
R^{2} & 0.465 & 0.462 
\end{array}
\]

\bigskip


\subsection*{Adding more features}

List of the features added back to the volatility model :

\begin{multicols}{2}
\begin{itemize}
  \item Present volatility
  \item open price
  \item close price
  \item high
  \item low
  \item volume
\end{itemize}
\end{multicols}

After implementation :
\bigskip
\[
\begin{array}{ccc}
Metric/Method & Base (vol) & With more features \\
\hline
MSE & 0.003 & 0.010\\
MAE & 0.040 & 0.079 \\
R^{2} & 0.322 & -0.996 
\end{array}
\]

\bigskip

From these results, it appear the volatility prediction actually gets worse when adding features. This can be due to the model being to small and not capturing the complex relationship, or, the features not being the most relevant (see correlation matrix).


\subsection*{Probabilistic model}
In addition of doing a regression on the volatility, also predict the parameters of the distribution, $(\mu, \sigma)$
With target set to next day $(\mu, \sigma)$ results gives :
\bigskip
\[
\begin{array}{ccc}
Metric/Target & mu & sigma \\
\hline
MSE & 0.00007 & 0.0002\\
MAE & 0.006 & 0.010 \\
R^{2} & 0.727 & 0.822 
\end{array}
\]
\bigskip

  \begin{center}
  \includegraphics[width=0.9\textwidth]{img/mu_sig_pred.png}
  \end{center}




\subsection*{Back to option price model}
Model with input :
\begin{verbatim}
  df[['Open', 'volatility', 'High', 'Close', 'Volume', 'returns', 'EWMA_VM', 'yang_zhang']]
\end{verbatim}

\medskip
\noindent
One hidden layer of size 64, no EMD, sliding window size of 20.
Result :
\bigskip
\[
\begin{array}{cc}
Metric/Method & Base\_option\_model \\
\hline
MSE & 0.017 \\
MAE & 0.101 \\
R^{2} & 0.182  
\end{array}
\]

\bigskip


\begin{center}
\includegraphics[width=0.7\textwidth]{img/PVSA_opt_base_model.png}
\end{center}





\subsection*{Correlation matrix}
Relationship between input features and target (option price) using the BS model.
Sample of option prices can be ofund online, but for large samples, (days to week) they are pay to download

\begin{center}
\includegraphics[width=0.7\textwidth]{img/corr_matrix_opt.png}
\end{center}









\newpage
\section*{Iteration 16}
\begin{flushright}
June 02, 2025
\end{flushright}
\hrule
\vspace{0.2in}

%-------------------------------------------------------------
\subsection*{Change $R^2$ formula}
Bounding $R^2$ between 0 and 1 either by \begin{itemize}
  \item Normalizing, the issue is that it change the scale, so a 0 before can become a 0.5.
  \item Or, by taking \textbf{$max(0.0 ; R^2)$} the downside is the lost of the information "how bad the model is", in our case the model should stay above 0 and if it is 0, the information lost isn't to bad since we already know its really bad.
\end{itemize}

%\bigskip

%-----------------------------------------------------------
\subsection*{Try a bigger model for relationship complexity}
From a model of size :
\[
[Input; 64; output]
\]

Let's try a model of size : 
\[
[Input; 256; 128; output]
\]

Result :

\bigskip
\[
\begin{array}{ccc}
Metric/Method & Base & Bigger model \\
\hline
MSE & 0.003 & 0.003\\
MAE & 0.040 & 0.041 \\
R^{2} & 0.322 & 0.309 
\end{array}
\]

\bigskip

%--------------------------------------------------------------
\subsection*{Probabilistic model}


\subsubsection*{Density funciton of $\sigma$}

Calculating the sd of returns, then predicting the target means of the returns's sd and sd of the returns's sd.\\
The result given by a [64;32] model are :
\begin{center}
\includegraphics[width=0.9\textwidth]{img/mu_sig_pred_sig.png}
\end{center}

\bigskip
It appear the model have better result predicting the mean, wich isn't surprising, due to the more stable nature of this metric. However, for the sd, the results aren't as good as for the mean, we can observe a similarity between the returns's sd time serie and the retust given by the model, indicating harder prediction when volatility increases.

\begin{center}
\includegraphics[width=0.9\textwidth]{img/evo_rt_sd.png}
\end{center}


\bigskip

\subsubsection*{Price model with bernoulli distribution}


\begin{center}
\includegraphics[width=0.7\textwidth]{img/dist_mvt.png}
\end{center}

This confusion matrix indicates the modle choose 1 or "up" a lot more than it should, This can be caused by the more important presence, of 1 in the distribution. However, this show that the model overestimate that number.

\begin{center}
\includegraphics[width=0.7\textwidth]{img/conf_mat_price.png}
\end{center}

\bigskip

\subsubsection*{Confidence interval}
Assuming a normal distribution of the returns we can calculating CI with :

\[
\mathrm{CI}_{1-\alpha}
= \bar{x} \;\pm\; z_{1-\frac{\alpha}{2}}\;\frac{s}{\sqrt{n}}
\]

\[
\bigl[
\bar{x} - z_{1-\frac{\alpha}{2}}\;\tfrac{s}{\sqrt{n}}
\;,\;
\bar{x} + z_{1-\frac{\alpha}{2}}\;\tfrac{s}{\sqrt{n}}
\bigr]
\]


Result to :
\begin{verbatim}
n         = 5369
mean      = 0.5866
std       = 0.0477
95% CI    = [0.5854, 0.5879]
\end{verbatim}


\bigskip

%-------------------------------------------------------------
\subsection*{Adding more features}

\bigskip
\[
\begin{array}{ccc}
Metric/Method & Base & With more features \\
\hline
MSE & 0.003 & 0.007\\
MAE & 0.040 & 0.067 \\
R^{2} & 0.322 & 0.000 
\end{array}
\]
\bigskip

Adding more featured worsten the performances compare to a simple time serie. On explenation can be a noisy data that confuses the model compare to the volatility data that can be noisy but with only one feature dim, this can be easier for the model. Also we can assume other data type could be more relevant, for exemple macro-economic data like news analysis, but this it out of scope.\\

Other type of numerical data could be relevant but adding too much features seems to lead to worse results.


\bigskip

%------------------------------------------------------------
\subsection*{Option price model with enhance volatility model + benchmark}


\bigskip
\[
\begin{array}{ccc}
Metric/Method & Base & vol\_LSTM\_EMD \\
\hline
MSE & 0.022 & 0.017\\
MAE & 0.121 & 0.102 \\
R^{2} & 0.000 & 0.000 
\end{array}
\]
\bigskip

Around $20\%$ improvement of the MSE\\
Same for MAE\\
$R^2$ to investigate...
\bigskip
%----------------------------------------------------------
\subsection*{Hyper-parameters tunning}


\begin{verbatim}
param_grid = {
    'hidden_dim1': [128, 64],
    'hidden_dim2': [64, 32],
    'lr': [1e-3, 1e-4],
    'weight_decay': [1e-5, 1e-6],
    'dropout': [0.1, 0.2]
}
\end{verbatim}

Not enough time yet to do the computation 



\end{document}
